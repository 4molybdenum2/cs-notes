Guest 1 should have its own TLB
Guest 2 should have its own TLB
TLB should be virtualized

If TLB miss is handled by software, virtualization is easy, because it is trapped by VMM
**Challenge:** But if TLB miss is hardware handled, VMM doesn't know if guest OS is updating TLB or not

Two level of mapping for address translation:

VPN                             -> PNP                            -> MPN
(guest OS page table)      (VMM Page Table)

For software managed TLB - memory virtualization becomes trivial

- Modern architecture support hardware level TLB, where hardware walks the page table on TLB miss
- Hypervisor can't intercept TLB misses from guest OS

Possible solutions:
- **Shadow paging** (https://stackoverflow.com/a/9834730/12201973https://stackoverflow.com/a/9834730/12201973):
	- Somehow hypervisor should let the TLB hold VPN -> MPN
	- Software based solution -> shadown paging / shadow page table
	- vmm iterates over guest page table and creates a shadow page table
	- in shadow page table - every guest address is translated into host address (machine address)
	- finally vmm sets CR3 register to point to the shadow page table
	- **how does hypervisor knows guest OS updates its guest page table?**
		- we need to sync shadow page table with guest page table
	- solution:
		- mark guest pages as **read only** - in shadow page table
		- if guest OS tries to modify its page tables, permission mismatch, it triggers page fault
		- VMM handles page fault by updating shadow page table


- Read dealing with page faults from the slides


```
Shadow page tables are used by the hypervisor to keep track of the state in which the guest "thinks" its page tables should be. The guest can't be allowed access to the hardware page tables because then it would essentially have control of the machine. So, the hypervisor keeps the "real" mappings (guest virtual -> host physical) in the hardware when the relevant guest is executing, and keeps a representation of the page tables that the guest thinks it's using "in the shadows," or at least that's how I like to think about it.

Notice that this avoids the GVA->GPA translation step.

As far as page faults go, nothing changes from the _hardware's_ point of view (remember, the hypervisor makes it so the page tables used by the hardware contain GVA->HPA mappings), a page fault will simply generate an exception and redirect to the appropriate exception handler. However, when a page fault occurs while a VM is running, this exception can be "forwarded" to the hypervisor, which can then handle it appropriately.

The hypervisor must build up these shadow page tables as it sees page faults generated by the guest. When the guest writes a mapping into one of its page tables, the hypervisor won't know right away, so the shadow page tables won't instantly "be in sync" with what the guest intends. So the hypervisor will build up the shadow page tables in, e.g., the following way:

- Guest writes a mapping for VA `0xdeadbeef` into it's page tables (a location in memory), but remember, this mapping isn't being used by the hardware.
- Guest accesses `0xdeadbeef`, which causes a page fault because the real page tables haven't been updated to add the mapping
- Page fault is forwarded to hypervisor
- Hypervisor looks at guest page tables and notices they're different from shadow page tables, says "hey, I haven't created a real mapping for `0xdeadbeef` yet"
- So it updates its shadow page tables and creates a corresponding `0xdeadbeef`->HPA mapping for the hardware to use.

The previous case is called a _shadow page fault_ because it is caused solely by the introduction of memory virtualization. So the handling of the page fault will stop at the hypervisor and the guest OS will have no idea that it even occurred. Note that the guest can also generate genuine page faults because of mappings it hasn't tried to create yet, and the hypervisor will forward these back up into the guest. Also realize that this entire process implies that _every_ page fault that occurs while the guest is executing must cause an exit to the VMM so the shadow page tables can be kept fresh. This is expensive, and one of the reasons why hardware support was introduced for memory virtualization -> extended page tables.
```



**Pros and Cons of Shadow Paging:**

Pros:
1. memory access are very fast when shadow page table is created
Cons:
1. maintaining consistency between GPT and SPT involve traps which is costly
2. TLB flush on every world switch
3. memory space overhead to maintain pmap




**Virtualizing Hardware Managed TLB**



Extended Page Table:
1. EPT: a page table with PPN -> MPN mapping referenced by EPT
2. CR3 pointer -> GPT
3. Hardware directly walks GPT + EPT (for each PPN access during GPT walk,needs to walk the EPT to determine MPN) - first looks up GPT then EPT, hardware knows now
4. No VM exits due to page faults, CR3 accesses or INVLPG



Pros and Cons of EPT:

Pros:
1. Simplified VMM design
2. Guest PT changes do not trap, minimize VM exits
3. Lower memory space over head (no need for pmap in memory)

Cons:
1. TLB miss is costly: can require many memory accesses to finish the walk





VMM Memory Management:
1. **Reclaiming:**
	1. Allow overcommitment of memory, total memory size can exceed actual machine memory size
	2. must have a way to reclaim memory, by swapping out to disk
	3. ESX does this
	4. traditional: add swap layer
	5. ESX asks guest OS to install balloon driver
	6. Inflate (creates pressure): allocate pinned pages -> guest operating system starts losing physical pages. From hypervisor's perspective -> hypervisor gets more machine pages (reclaimed) coz Guest operating system is not using it.
	7. Deflate: (decreases pressure) -> opposite
Idea is instead of letting the guest OS and hypervisor communicate directly, manage memory with the help of a device driver (indirect influence decision of guest OS)



1. **Sharing:**
	1. Multiple VMs running same OS, same code, same apps, redundant code, duplicated data
	2. transparent page sharing:
		1. map multiple PPN's to single MPN (copy-on-write)
		2. requires guest OS hooks
	3. ESX content based sharing:
		1. general purpose no guest OS changes required
		2. background  activity saves memory over time

For each physical page -> hashing the content -> maintain hash table -> if successful match then same content -> allow sharing


1. **Allocation:**
Read slides


Tax on idle memory:
1. charge more for idle pages than active pages
2. idle adjusted shares-per-page ratio

Tax rate:
1. adjustable parameter

High default rate:
- reclaim more idle memory
- some buffer against rapid working set increase

